<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llms on Armur</title>
    <link>http://localhost:1313/blogs/tags/llms/</link>
    <description>Recent content in Llms on Armur</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 15:20:22 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/blogs/tags/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using LLMs in Code Security and Static Analysis</title>
      <link>http://localhost:1313/blogs/posts/using-llms-in-code-security-and-static-analysis/</link>
      <pubDate>Thu, 18 Jul 2024 15:20:22 +0530</pubDate>
      <guid>http://localhost:1313/blogs/posts/using-llms-in-code-security-and-static-analysis/</guid>
      <description>Introduction One of the fundamental building blocks of software development is code quality. High-quality code is directly linked to secure, stable, and reliable applications. Traditionally, static analysis has been a key technique to maintain this quality, but recent advancements have introduced a transformative element: Large Language Models (LLMs). These advanced AI systems are now playing a major role in enhancing code security and ensuring issues are identified before software is released into production.</description>
    </item>
  </channel>
</rss>
